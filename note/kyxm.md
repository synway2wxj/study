* https://github.com/roncoo/roncoo-pay?spm=a2c4e.11153940.blogcont69572.17.69e5510asIfpxV
* https://blog.csdn.net/chendaoqiu/article/details/53117870
* Apache POI  处理office文档
* IText    PDF操作
* Base64编码
* pingyin4j  中文转拼音库
* Commons-IO,处理IO
* Commons-beanutils 用来处理javaBean类的反射
* Commons-codec 处理常用加密编码
Commons-collections 对各种集合类
patchca Java验证码
Commons Configuration Java配置文件
cglib java动态代理
com4j 调用com的类库
jsaparjava文本文件处理
dregexp 正则
jegg java多线程开发包
apache mina
jbossnetty
httpclient
boogle guice ioc框架
jboss jbpm工作流引擎开源框架
Drools规则引擎开源框架
date4j
dbunit 数据库测试框架
mockito java mocking框架
joda time 强大易用的日期和时间库


* RocketMq

===
* 应用耦合，异步消息，流量削锋;高性能，高可用，可伸缩和最终一致性架构
* ActiveMQ，RabbitMQ，ZeroMQ，Kafka，MetaMQ，RocketMQ
* 消息队列在实际应用中常用的使用场景。异步处理，应用解耦，流量削锋和消息通讯四个场景
* 订单系统：用户下单后，订单系统完成持久化处理，将消息写入消息队列，返回用户订单下单成功
* 库存系统：订阅下单的消息，采用拉/推的方式，获取下单信息，库存系统根据下单信息，进行库存操作
* 秒杀活动，一般会因为流量过大，导致流量暴增，应用挂掉。为解决这个问题，一般需要在应用前端加入消息队列
* 日志处理是指将消息队列用在日志处理中，比如Kafka的应用，解决大量日志传输的问题
* (1)Kafka：接收用户日志的消息队列。

(2)Logstash：做日志解析，统一成JSON输出给Elasticsearch。

(3)Elasticsearch：实时日志分析服务的核心技术，一个schemaless，实时的数据存储服务，通过index组织数据，兼具强大的搜索和统计功能。

(4)Kibana：基于Elasticsearch的数据可视化组件，超强的数据可视化能力是众多公司选择ELK stack的重要原因
* Zookeeper注册中心，日志收集客户端，Kafka集群和Storm集群（OtherApp）四部分
* 中文分词插件elasticsearch-analysis-ik
* 提高系统性能首先考虑的是数据库的优化
* 数据库的使用你可能忽略了这些
* 尽量把流量都挡在数据库之前
* 阻隔直达数据库的流量，缓存组件和消息组件是两大杀器
* 持久化确保MQ的使用不只是一个部分场景的辅助工具，而是让MQ能像数据库一样存储核心的数据
* 在现在大流量、大数据的使用场景下，只支持单体应用的服务器软件基本是无法使用的，支持分布式的部署，才能被广泛使用
* MQ 数据是只有一条数据在使用中。 在很多存在并发，而又对数据一致性要求高，而且对性能要求也高的场景，如何保证，那么MQ就能起这个作用了。不管多少流量进来，MQ都会让你遵守规则，排除处理，不会因为其他原因，导致并发的问题，而出现很多意想不到脏数据
* 分布式事务是我们开发中一直尽量避免的一个技术点，但是，现在越来越多的系统是基于微服务架构开发，那么分布式事务成为必须要面对的难题，解决分布式事务有一个比较容易理解的方案，就是二次提交。基于MQ的特点，MQ作为二次提交的中间节点，负责存储请求数据，在失败的情况可以进行多次尝试，或者基于MQ中的队列数据进行回滚操作，是一个既能保证性能，又能保证业务一致性的方案
* 当用户注册成功过后，通过MQ通知其他业务进行操作。确保注册用户的性能
* 后台发布商品的时候，商品数据需要从数据库中转换成搜索引擎数据（基于elasticsearch），那么我们应该将商品写入数据库后，再写入到MQ，然后通过监听MQ来生成elasticsearch对应的数据
* 用户下单后，24小时未支付，需要取消订单。以前我们可能是定时任务循环查询，然后取消订单。实际上，我更推荐类似延迟MQ的方式，避免了很多无效的数据库查询，将一个MQ设置为24小时后才让消费者消费掉，这样很大程度上能减轻服务器压力
* 支付完成后，需要及时的通知子系统（进销存系统发货，用户服务积分，发送短信）进行下一步操作，但是，支付回调我们都是需要保证高性能的，所以，我应该直接修改数据库状态，存入MQ，让MQ通知子系统做其他非实时的业务操作
* 任何一个技术的出现，都有他的业务场景，只有清楚技术的特点，才能更加贴切的挖掘出应用场景，深入思考，深入实践才能将一个技术用在最合适的地方
* 数据库的管理是一个非常专业的事情，对数据库的调优、监控一般是由数据库工程师完成，但是开发人员也经常与数据库打交道，即使是简单的增删改查也是有很多窍门
* 字段长度对索引的影响是很大的
* 字符串字段长度都差不多的，可以预估长度的，用char
字符串长度差异大，用varchar,限制长度，不要浪费空间
整型根据大小，选择合适的类型
时间建议用timestamp
建议使用decimal，不建议使用float,如果是价格，可以考虑用int或bigint，如1元，存储的就是100
* 大表减少联表，最好是单表查询
* 合理的冗余字段
配合内存数据库（redis\mongodb）使用
联表变多次查询（下文会有说明）
* 如果考虑都后期数据量大，需要分表分库，就应该尽早实时单表查询，现在的数据库分表分库的中间件基本都无法支持联表查询
* 一个表的索引不宜过多，建议最多就5个，索引不可能满足所有的场景，但是了个满足绝大部分的场景
* 聚合操作如count,group等，是数据库性能的大杀手，经常会出现大面积的表扫描和索表的情况
* 避免聚合操作的方法就是将实时的count计算结果用字段去存储，去累加这个结果。当然，也可以考虑用spark等实时计算框架去处理，这种高深的技术
* 阿里SQL的规范有很多可以吸取的地方
* LAMP经典架构(linux+apache+mysql+php)
* web机器+数据库服务器
* PHP能完成从页面渲染到数据访问
* 花了大量的时间重构，把asp网站改版为.net，最后还得花更大的经历和代价改版为java版本
* 没有过多考虑技术是否牛逼、没有过多考虑性能是否海量、没有考虑稳定性如何
* 最高每日 31 万PV
* 数据库和应用互相影响
* 当时淘宝的情况， PHP 语言来说它是放在 Apache 上的，每一个请求都会对数据库产生一个连接。php的连接池只能从第三方扩展里实现连接池
* 数据库的访问压力过大，有什么对应的方法减少数据库端的开销变得及其重要
* 采用oracle替换mysql，解决高并发读数据等系列问题，也引入了数据库连接池，以及读写分离
* 最后，为了配合Oracle，php也彻底被替换为java。替换为java其实还有一个很重要的原因：配合数据库连接池的使用，毕竟php的代理连接池坑太多
* 第一个，往集群方向发展，从硬件的角度可以横向扩展应用服务器
* 为了减少对数据库端的访问压力，把访问转移到缓存上，也就涉及到了分布式缓存,eg:memcached等开源架构
* 也是为了减少数据库端的压力，把小文件存储转换到便宜的硬件上，也就有了分布式存储，后来的TFS
* 最终你会发现，都是为了缓解数据库端的压力
* 代码工程臃肿到了极致，严重影响了开发速度和发布
* 上百人维护一个代码百万行的核心工程denali。多个业务系统中的超过1/3的核心代码重复编写
* 一群人在上面拉分支，发布上线，然后在合并分支。线上出问题了，还得回滚，再合并，特别是开发、SCM等痛苦死了
* 数据库端的压力一直是重中之重，小型机也扛不住了
* 数据库服务器的CPU占用超过90%，说明开的数据库连接也达到了极限，数据库机随时崩溃
* 雪崩效应，90%都是来源于数据库端的压力。数据库端的压力会转移到应用端，最后互相死循环。最重要的就是要管住数据库端的压力，并且把应用端的压力与数据库端做切割。切割有多种方法，最好的方法就是物理隔离
* 数据库端需要做垂直拆分了，按照业务线
* 应用端的代码结构也需要做垂直拆分了，按照业务垂直拆分代码
* 接口需要清理依赖关系，是否需要单独部署？需要一个支撑高并发的通讯框架
* 中间件需要形成自己的矩阵了，分布式缓存、分布式存储、SQL路由
* 自己的一套SOA之路
* 网站的流量和交易数据
* 流量峰值和全天的交易总额
* 网站的各种监控指标，适时的调整机器和增减功能
* 能办得起来如此盛宴者，需要强大的财力物力、组织能力、技术实力
* 一定要是“分布式的”、“可复制的”、“可扩展的”，洗菜切菜要有“工作流引擎”，上菜的路径要用图论来计算出来
* 拥有全国最大的 Hadoop 分布式计算集群之一，日新增数据 50TB，有 40PB 海量数据存储。分布在全国各地 80 多个节点的 CDN 网络，支持的流量超过 800Gbps。淘宝的搜索引擎能够对数十亿的商品数据进行实时搜索，另外还拥有自主研发的文件存储系统和缓存系统，以及 Java 中间件和消息中间件系统，这一切组成了一个庞大的电子商务操作系统
* 如果你在网上购买过火车票的话，更能体会到网站能支持多大的流量有多重要。但这不是一朝一夕做出来的，也不是有钱就能办到的
* 在什么样的阶段采用什么样的技术。在发展的过程中网站会遇到各种各样的问题和业务带来的压力，正是这些原因才推动着技术的进步和发展，而技术的发展又会反过来促进业务的更大提升
* 淘宝网的系统也从使用一台服务器，到采用万台以上的服务器
* 好的架构是进化来的，不是设计来的
* 好的架构图充满美感”
* 看了很多系统的架构，
* 给业务分模块，一个模块一个模块的替换 ，开发完毕之后放到不同的应用集群上
* Java 应用服务器是 Weblogic，MVC 框架是 WebX、控制层用了 EJB、持久层是 iBATIS，另外为了缓解数据库的压力，商品查询和店铺查询放在搜索引擎上面。
* 开发语言本身都不是系统的瓶颈，业务带来的压力更多的是压到了数据和存储上
*  IBM 的小型机、Oracle 的数据库、EMC 的存储
* 花钱买豪华的配置，也许能支持 1 亿 PV 的网站，但淘宝网的发展实在是太快了，到了 10 亿怎么办？到了百亿怎么办？在 N 年以后，我们不得不创造技术，解决这些只有世界顶尖的网站才会遇到的问题。后来我们在开源软件的基础上进行自主研发，一步一步的把 IOE（IBM 小型机、Oracle、EMC 存储）这几个“神器”都去掉了
* 带着一帮 DBA 在优化 SQL 和存储，行癫带着几个架构师在研究数据库的扩展性
* Oracle 本身是一个封闭的系统，用 Oracle 怎么做扩展？用现在一个时髦的说法就是做“分库分表”
* 一台 Oracle 的处理能力是有上限的，它的连接池有数量限制，查询速度跟容量成反比。简单的说，在数据量上亿、查询量上亿的时候，就到它的极限了。要突破这种极限，最简单的方式就是多用几个 Oracle 数据库。但一个封闭的系统做扩展，不像分布式系统那样轻松。我们把用户的信息按照 ID 来放到两个数据库里面（DB1/DB2），把商品的信息跟着卖家放在两个对应的数据库里面，把商品类目等通用信息放在第三个库里面(DBcommon)。这么做的目的除了增加了数据库的容量之外，还有一个就是做容灾，万一一个数据库挂了，整个网站上还有一半的数据能操作
* 数据库这么分了之后，应用程序有麻烦了，如果我是一个买家，买的商品有 DB1 的也有 DB2 的，要查看“我已买到的宝贝”的时候，应用程序怎么办？
* 数据库这么分了之后，应用程序有麻烦了，如果我是一个买家，买的商品有 DB1 的也有 DB2 的，要查看“我已买到的宝贝”的时候，应用程序怎么办？必须到两个数据库里面分别查询出来对应的商品。要按时间排序怎么办？两个库里面“我已买到的宝贝”全部查出来在应用程序里面做合并。还有分页怎么处理？关键字查询怎么处理？这些东西交给程序员来做的话会很悲催，于是行癫在淘宝的第一个架构上的作品就来解决了这个问题，他写了一个数据库路由的框架 DBRoute，这个框架在淘宝的 Oracle 时代一直在使用。后来随着业务的发展，这种分库的第二个目的 —— 容灾的效果就没有达到。像评价、投诉、举报、收藏、我的淘宝等很多地方，都必须同时连接 DB1 和 DB2，哪个库挂了都会导致整个网站挂掉
* 在 2005、2006年的时候，Spring 大放异彩，正好利用 Spring 的反射（IoC）模式替代了 EJB 的工厂模式，给整个系统精简了很多代码
* 为了减少数据库的压力，提高搜索的效率，我们引入了搜索引擎。随着数据量的继续增长，到了 2005 年，商品数有 1663 万，PV 有 8931 万，注册会员有 1390 万，这给数据和存储带来的压力依然山大，数据量大，性能就慢。亲，还有什么办法能提升系统的性能？一定还有招数可以用，这就是缓存和 CDN（内容分发网络）
* 你可以想象，九千万的访问量，有多少是在商品详情页面？访问这个页面的时候，数据全都是只读的（全部从数据库里面读出来，不写入数据库），如果把这些读操作从数据库里面移到内存里，数据库将会多么的感激涕零。在那个时候我们的架构师多隆大神，找到了一个基于 Berkeley DB 的开源的缓存系统，把很多不太变动的只读信息放了进去
* 这个字段太大了，查询商品信息的时候很多都不需要查看详情，它跟商品的价格、运费这些放在一个表里面，拖慢了整个表的查询速度
* 搭建了淘宝自己的 CDN 网络
* 淘宝的 CDN 系统支撑了 800Gbps 以上的流量
* ChinaCache 在全国 50 多个大中城市拥有近 300 个节点，全网处理能力超过 500Gbps，其 CDN 网络覆盖中国电信、中国网通、中国移动、中国联通、中国铁通和中国教育科研网等各大运营商
* CDN 需要大量的服务器，要消耗很多能源（消耗多少？在前两年我们算过一笔帐，淘宝上产生一个交易，消耗的电足以煮熟 4 个鸡蛋）
* 数据库的内容更新的时候，忘记通知缓存系统，结果在测试的时候就发现我改过的数据怎么在页面上没变化呢。后来做了一些页面上的代码，修改 CSS 和 JS 的时候，用户本地缓存的信息没有更新，页面上也会乱掉，在论坛上被人说的时候，我告诉他用 Ctrl+F5 刷新页面，然后赶紧修改脚本文件的名称，重新发布页面
* 已经有几百台应用服务器了，这上面的 Java 应用服务器是 WebLogic，而 WebLogic 是非常贵的，比这些服务器本身都贵。有一段时间多隆研究了一下 JBoss，说我们换掉 WebLogic 吧，于是又省下了不少银两
* 对数据分库、放弃 EJB、引入 Spring、加入缓存、加入 CDN、采用开源的 JBoss  提高容量、提高性能、节约成本
* 有很多优秀的人才加入，也开发了很多优秀的产品
* 大规模的小文件存储与读取，因为磁头需要频繁的寻道和换道，因此在读取上容易带来较长的延时。在大量高并发访问量的情况下，简直就是系统的噩梦
* 从 2006 年开始，淘宝网决定自己开发一套针对海量小文件存储的文件系统，用于解决自身图片存储的难题
* 研发过程中，将开源和自主开发相结合，会有更好的可控性，系统出问题了，完全可以从底层解决问题，系统扩展性也更高
* 2007 年他们公布了 GFS（ Google File System ）的设计论文，这给我们带来了很多借鉴的思路。随后我们开发出了适合淘宝使用的图片存储系统TFS（Taobao File System）
* 文件比较小；并发量高；读操作远大于写操作；访问随机；没有文件修改的操作；要求存储成本低；能容灾能备份。应对这种需求，显然要用分布式存储系统；由于文件大小比较统一，可以采用专有文件系统；并发量高，读写随机性强，需要更少的 IO 操作；考虑到成本和备份，需要用廉价的存储设备；考虑到容灾，需要能平滑扩容。
* 集群由一对 Name Server 和多台 Data Serve r构成，Name Server 的两台服务器互为双机
* 每个 Data Server 运行在一台普通的 Linux 主机上
　　• 以 block 文件的形式存放数据文件(一般64M一个block )
　　• block 存多份保证数据安全
　　• 利用 ext3 文件系统存放数据文件
　　• 磁盘 raid5 做数据冗余
　　• 文件名内置元数据信息，用户自己保存 TFS 文件名与实际文件的对照关系 – 使得元数据量特别小
  * 而对于淘宝网的用户来说，图片文件究竟用什么名字来保存实际上用户并不关心，因此TFS 在设计规划上考虑在图片的保存文件名上暗藏了一些元数据信息，例如图片的大小、时间、访问频次等等信息，包括所在的逻辑块号。而在元数据上，实际上保存的信息很少，因此元数据结构非常简单。仅仅只需要一个 fileID，能够准确定位文件在什么地方。
  * 由于大量的文件信息都隐藏在文件名中，整个系统完全抛弃了传统的目录树结构，因为目录树开销最大。拿掉后，整个集群的高可扩展性极大提高。实际上，这一设计理念和目前业界的“对象存储”较为类似，淘宝网 TFS 文件系统已经更新到 1.3 版本，在生产系统的性能已经得到验证，且不断得到了完善和优化，淘宝网目前在对象存储领域的研究已经走在前列。
  * 到现在为止，淘宝网给每个用户提供了 1G 的图片空间
  * 在系统发展的过程中，架构师的眼光至关重要，作为程序员，把功能实现即可，但作为架构师，要考虑系统的扩展性、重用性，这种敏锐的感觉，有人说是一种代码洁癖
* Webx 是一个扩展性很强的框架，行癫在这个框架上插入了数据分库路由的模块、session 框架
* 从系统的角度来看，我们建立了“属性”这样一个数据结构，由于除了类目的子节点有属性，父节点也有可能有属性，于是类目属性合起来也是一个结构化的数据对象。这个做出来之后我们把它独立出来作为一个服务，叫做 catserver（category server）。跟类目属性密切关联的商品搜索功能，独立出来，叫做 hesper（金星），catserver 和 hesper 供淘宝的前后台系统调用
* 淘宝前台系统的业务量和代码量还是爆炸式的增长了起来。业务方总在后面催，开发人员不够了就继续招人，招来的人根本看不懂原来的业务，只好摸索着在“合适的地方”加一些“合适的代码”，看看运行起来像那么回事，就发布上线了
* 一切要以稳定为中心，所有影响系统稳定的因素都要解决掉
* 例如把回归测试日常化，每天晚上都跑一遍整个系统的回归。还有就是在这种要求下，我们不得不对这个超级复杂的系统做肢解和重构，其中复用性最高的一个模块 —— 用户信息模块开始拆分出来了，我们叫它 UIC（user information center）。在 UIC 里面，它只处理最基础的用户信息操作，例如getUserById、getUserByName等等。　　
* 从查询商品、购买商品、评价反馈、查看订单这一整个流程都重新写了一套出来。
* 类目属性、用户中心、交易中心，随着这些模块逐步的拆分和服务化改造，我们在系统架构方面也积累了不少的经验。到 2008 年底干脆做了一个更大的项目，把淘宝所有的业务都模块化，这是继 2004 年从 LAMP 架构到 Java 架构之后的第二次脱胎换骨
* 其中 UIC 和 Forest 上文说过，TC、IC、SC分别是交易中心（Trade Center）、商品中心（Item Center）、店铺中心（Shop Center），这些中心级别的服务只提供原子级的业务逻辑，如根据ID查找商品、创建交易、减少库存等操作。再往上一层是业务系统TM（Trade Manager交易业务）、IM（Item Manager商品业务）、SM（Shop Manager，因为不好听，所以后来改名叫 SS：Shop System，店铺业务）、Detail（商品详情）
* 拆分之后，系统之间的交互关系变得非常复杂
* 系统这么拆分的话，好处显而易见，拆分之后每个系统可以单独部署，业务简单，方便扩容；有大量可重用的模块以便于开发新的业务；能够做到专人专事，让技术人员更加专注于某一个领域。这样要解决的问题也很明显，分拆之后，系统之间还是必须要打交道的，越往底层的系统，调用它的客户方越多，这就要求底层的系统必须具有超大规模的容量和非常高的可用性。另外，拆分之后的系统如何通讯？这里需要两种中间件系统，一种是实时调用的中间件（淘宝的HSF，高性能服务框架）、一种是异步消息通知的中间件（淘宝的Notify）。另外还有一个需要解决的问题是用户在A系统登录了，到B系统的时候，用户的登录信息怎么保存？这又涉及到一个 Session 框架
* LVS(Linux Virtual Server)，世界上最流行的负载均衡系统之一
* 你的浏览器在同一个域名下并发加载的资源数量是有限制的，例如IE6-7是两个，IE8是6个，Chrome各版本不大一样，一般是4-6个。我刚刚看了一下，我访问淘宝网首页需要加载126个资源，那么如此小的并发连接数自然会加载很久。所以前端开发人员往往会将上述这些资源文件分布在好多个域名下，变相的绕过浏览器的这个限制，同时也为下文的CDN工作做准备
* 淘宝在全国各地建立了数十上百个CDN节点，利用一些手段保证你访问的（这里主要指js、css、图片等）地方是离你最近的CDN节点，这样便保证了大流量分散在各地访问的加速节点上
* 假若一个卖家发布了一个新的宝贝，上传了几张新的宝贝图片，那么淘宝网如何保证全国各地的CDN节点中都会同步的存在这几张图 片供用户使用呢？这里边就涉及到了大量的内容分发与同步的相关技术。淘宝开发了分布式文件系统TFS(Taobao File System)来处理这类问题
* 主搜索系统便开始为你服务了。它首先对你输入的内容基于一个分词库进行分词操作。众所周知，英文是以词为单位的，词和词之间是靠空格隔开，而中文是以字为单位，句子中所有的字连起来才能描述一个意思
* 主搜索系统便根据上述以及更多复杂的条件列出了搜索结果，这一切是由一千多台搜索服务器完成。
* 你开始查看宝贝详情页面。经常网购的亲们会发现，当你买过了一个宝贝之后，即便是商家多次修改了宝贝详情页，你仍然能够通过‘已买到的宝贝’查看当时的快照。这是为了防止商家对在商品详情中承诺过的东西赖账不认。那么显然，对于每年数十上百亿比交易的商品详情快照进行保存和快速调用不是一个简单的事情。这 其中又涉及到数套系统的共同协作，其中较为重要的是Tair，淘宝自行研发的分布式KV存储方案
* 快速及时 传输同步这些日志数据，淘宝研发了TimeTunnel，用于进行实时的数据传输，交给后端系统进行计算报表等操作
* 如此巨大的数据量经过淘宝系统1:120的极限压缩存储在淘宝的数据仓库中。并且通过一个叫做云梯的，由2000多台服务器组成的超大规模数据系统不断的进行分析和挖掘
* 从这些数据中淘宝能够知道小到你是谁，你喜欢什么，你的孩子几岁了，你是否在谈恋爱，喜欢玩魔兽世界的人喜欢什么样的饮料等，大到各行各业的零售情况、各类商品的兴衰消亡等等海量的信息
* 即便是你仅仅访问一次淘宝的首页，所涉及到的技术和系统规模都是你完全无法想 象的，是淘宝2000多名顶级的工程师们的心血结晶，其中甚至包括长江学者、国家科学技术最高奖得主等众多大牛。同样，百度、腾讯等的业务系统也绝不比淘宝简单。你需要知道的是，你每天使用的互联网产品，看似简单易用，背后却凝聚着难以想象的智慧与劳动
* 前端系统双11峰值有效请求约60w以上的QPS ，而后端cache的集群峰值近2000w/s、单机也近30w/s
* 数据隔离。 秒杀所调用的数据大部分都是热数据，比如会启用单独cache集群或MySQL数据库来放热点数据，目前也是不想0.01%的数据影响另外99.99%。
* 对大流量系统的数据做分层校验也是最重要的设计原则，所谓分层校验就是对大量的请求做成“漏斗”式设计，如图3所示：在不同层次尽可能把无效的请求过滤，“漏斗”的最末端才是有效的请求，要达到这个效果必须对数据做分层的校验
* 先做数据的动静分离

将90%的数据缓存在客户端浏览器

将动态请求的读数据Cache在Web端

对读数据不做强一致性校验

对写数据进行基于时间的合理分片

对写请求做限流保护

对写数据进行强一致性校验
* 其实Java和通用的Web服务器相比（Nginx或Apache）在处理大并发HTTP请求时要弱一点，所以一般我们都会对大流量的Web系统做静态化改造，让大部分请求和数据直接在Nginx服务器或者Web代理服务器（Varnish、Squid等）上直接返回（可以减少数据的序列化与反序列化），不要将请求落到Java层上，让Java层只处理很少数据量的动态请求
* 直接使用Servlet处理请求。 避免使用传统的MVC框架也许能绕过一大堆复杂且用处不大的处理逻辑，节省个1ms时间
* 直接输出流数据。 使用resp.getOutputStream()而不是resp.getWriter()可以省掉一些不变字符数据编码，也能提升性能；还有数据输出时也推荐使用JSON而不是模板引擎（一般都是解释执行）输出页面
* 虽然我们的Tair缓存机器单台也能支撑30w/s的请求，但是像大秒这种级别的热点商品还远不够，那如何彻底解决这种单点瓶颈？答案是采用应用层的Localcache，即在秒杀系统的单机上缓存商品相关的数据，如何cache数据？也分动态和静态
* 可以允许一定的脏数据，因为这里的误判只会导致少量一些原本已经没有库存的下单请求误认为还有库存而已，等到真正写数据时再保证最终的一致性。这样在数据的高可用性和一致性做平衡来解决这种高并发的数据读取问题
* 解决大并发读问题采用Localcache和数据的分层校验的方式，但是无论如何像减库存这种大并发写还是避免不了，这也是秒杀这个场景下最核心的技术难题。
* 同一数据在数据库里肯定是一行存储（MySQL），所以会有大量的线程来竞争InnoDB行锁，当并发度越高时等待的线程也会越多，TPS会下降RT会上升，数据库的吞吐量会严重受到影响。说到这里会出现一个问题，就是单个热点商品会影响整个数据库的性能，就会出现我们不愿意看到的0.01%商品影响99.99%的商品，所以一个思路也是要遵循前面介绍第一个原则进行隔离，把热点商品放到单独的热点库中。但是无疑也会带来维护的麻烦
* 不管做什么事情都喜欢进行总结的一个人
* 今天面试我问你static关键字有哪些作用，如果你答出static修饰变量、修饰方法我会认为你合格，答出静态块，我会认为你不错，答出静态内部类我会认为你很好，答出静态导包我会对你很满意，因为能看出你非常热衷研究技术
* （1）ConcurrentHashMap的锁分段技术
（2）ConcurrentHashMap的读是否要加锁，为什么
（3）ConcurrentHashMap的迭代器是强一致性的迭代器还是弱一致性的迭代器
* synchronized和ReentrantLock的区别、synchronized锁普通方法和锁静态方法、死锁的原理及排查方法
* 
* Java虚拟机应该是很重要的一块内容 要想拿高工资，JDK源码不可不读
* 谈谈分布式Session的几种实现方式？（常用的四种能答出来自然是让面试官非常满意的）
* 阿里巴巴内部的数据分布在各个独立的业务系统中，如：商品中心、交易平台、用户中心，各个独立系统间通过HSF（High-speed Service Framework）进行数据交换。如何将这些数据安全可控的开放给外部商家和ISV
* 千万级QPS全部打到DB是不可取的，尽管DB有做分库分表处理，所以我们在DB前面加了一层分布式缓存；然而千万级QPS需要近百台缓存服务器，为了节约缓存服务器开销以及减少过多的网络请求，我们在分布式缓存前面加了一层LRU规则的本地缓存；为了防止缓存被击穿，我们在本地缓存前面加了一层BloomFilter
* Tair  TFS的开发,让淘宝的图片功能得到了充分的发挥
* diamond为应用系统提供了获取配置的服务，应用不仅可以在启动时从diamond获取相关的配置，而且可以在运行中对配置数据的变化进行感知并获取变化后的配置数据
